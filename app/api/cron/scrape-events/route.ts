import { NextRequest, NextResponse } from "next/server";
import { createServiceClient } from "@/lib/supabase/service";

/**
 * V√©rifie que la requ√™te vient bien de Vercel Cron
 */
function verifyCronRequest(request: NextRequest): boolean {
  const authHeader = request.headers.get("authorization");
  const cronSecret = process.env.CRON_SECRET;
  
  if (!cronSecret) {
    console.error("‚ùå CRON_SECRET n'est pas d√©fini dans les variables d'environnement");
    return false;
  }
  
  return authHeader === `Bearer ${cronSecret}`;
}

/**
 * GET /api/cron/scrape-events
 * 
 * Cron job pour scraper automatiquement les √©v√©nements des organisateurs
 * Configur√© pour s'ex√©cuter toutes les heures (schedule: "0 * * * *")
 */
export async function GET(request: NextRequest) {
  // V√©rifier que la requ√™te vient bien de Vercel Cron
  if (!verifyCronRequest(request)) {
    return NextResponse.json(
      { error: "Non autoris√©" },
      { status: 401 }
    );
  }

  try {
    const supabase = createServiceClient();
    
    console.log("üîÑ D√©marrage du cron de scraping des √©v√©nements");
    
    // R√©cup√©rer tous les organisateurs avec une configuration de scraping
    const { data: organizers, error: orgError } = await supabase
      .from("organizers")
      .select("id, name, scraping_config")
      .not("scraping_config", "is", null);
    
    if (orgError) {
      console.error("‚ùå Erreur lors de la r√©cup√©ration des organisateurs:", orgError);
      return NextResponse.json(
        { 
          success: false, 
          error: orgError.message 
        },
        { status: 500 }
      );
    }
    
    if (!organizers || organizers.length === 0) {
      console.log("‚ÑπÔ∏è Aucun organisateur avec configuration de scraping trouv√©");
      return NextResponse.json({
        success: true,
        message: "Aucun organisateur √† scraper",
        scraped: 0,
        errors: 0,
        total: 0,
      });
    }
    
    console.log(`üìã ${organizers.length} organisateur(s) √† scraper`);
    
    let scraped = 0;
    let errors = 0;
    const errorDetails: string[] = [];
    
    // Scraper les √©v√©nements pour chaque organisateur
    for (const organizer of organizers) {
      try {
        console.log(`üìã Scraping pour ${organizer.name} (${organizer.id})...`);
        
        // TODO: Impl√©menter votre logique de scraping ici
        // Par exemple, appeler l'endpoint /api/events/scrape avec l'ID de l'organisateur
        
        // Exemple d'appel interne √† votre API de scraping :
        // const scrapeResponse = await fetch(
        //   `${process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000'}/api/events/scrape`,
        //   {
        //     method: 'POST',
        //     headers: { 'Content-Type': 'application/json' },
        //     body: JSON.stringify({ organizerId: organizer.id }),
        //   }
        // );
        
        scraped++;
      } catch (error: any) {
        console.error(`‚ùå Erreur lors du scraping pour ${organizer.name}:`, error);
        errors++;
        errorDetails.push(`${organizer.name}: ${error.message || "Erreur inconnue"}`);
      }
    }
    
    console.log(`‚úÖ Cron termin√©: ${scraped} r√©ussis, ${errors} √©checs`);
    
    return NextResponse.json({
      success: errors === 0,
      message: `Scraping termin√© pour ${organizers.length} organisateurs`,
      scraped,
      errors,
      total: organizers.length,
      errorDetails: errors > 0 ? errorDetails : undefined,
    });
    
  } catch (error: any) {
    console.error("‚ùå Erreur lors de l'ex√©cution du cron:", error);
    return NextResponse.json(
      {
        success: false,
        error: error.message || "Erreur inconnue",
      },
      { status: 500 }
    );
  }
}

